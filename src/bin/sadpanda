#!/usr/bin/env python3

"""
sadpanda.py
===========

Sadpanda script for downloading doujins and related stuff.

Dependencies
------------

* Python 3
* Beautiful Soup 4

Configuration
-------------

Create a file in your home directory called .sadpanda.cfg with the following:

    { "username": "sad", "passwd": "panda"}

You can also configure the timeouts manually in this file (Search for
time.sleep).  Sadpanda could block/ban if you spam too fast.

Usage
-----

    $ sadpanda <url>

url is the the url of the sadpanda manga

"""

import urllib.request
from urllib.parse import urlencode
from urllib.error import URLError
from http.cookiejar import CookieJar
import copy
import re
import os
import logging
import time
import argparse
import json

from bs4 import BeautifulSoup

# item: doujin pages
# page: displayed page on sadpanda site, which shows a number of items

# Definitions
logger = logging.getLogger(__name__)  # pylint: disable=invalid-name

FORUMS = "http://forums.e-hentai.org/index.php"
LOGIN = "http://forums.e-hentai.org/index.php?act=Login&CODE=01&CookieDate=1"
RESET = "http://exhentai.org/home.php"


def load_config():
    """Load configuration."""
    path = os.path.join(os.path.expanduser("~"), '.sadpanda.cfg')
    with open(path) as config:
        config = json.load(config)
    return config


def urlopen(*args, **kwargs):
    """Open URL and return response, retrying if error."""
    while True:
        try:
            return urllib.request.urlopen(*args, **kwargs)
        except URLError as err:
            logger.warning("Error %r; retrying...", err)
        time.sleep(1)


def convert_cookie(cookie):
    """Make sadpanda-fied copy of the cookie."""
    cookie = copy.deepcopy(cookie)
    cookie.domain = '.exhentai.org'
    return cookie


def get_total_pages(data):
    """Get the total number of pages from binary HTML data."""
    soup = BeautifulSoup(data.decode())
    maxpage = 1
    for element in soup.find_all('a'):
        try:
            onclick_val = element['onclick']
        except KeyError:
            continue
        if onclick_val == 'return false' and element.string != '>':
            maxpage = max(maxpage, int(element.string))
    return maxpage


def get_num_items(data):
    """Get the number of items on a page from binary HTML data."""
    soup = BeautifulSoup(data.decode())
    div = soup.find('div', {'id': 'gdt'})
    item_numbers = [int(img['alt']) for img in div.find_all('img')]
    return max(item_numbers) - min(item_numbers) + 1


def get_name(data):
    """Get name of doujin from binary HTML data"""
    soup = BeautifulSoup(data.decode())
    h1_element = soup.find(id='gn')
    return h1_element.string

_PAGE_LINK_PATTERN = re.compile(r'exhentai.org/s/')


def get_item_links(data):
    """Get links to items from binary HTML data"""
    soup = BeautifulSoup(data.decode())
    links = []
    for element in soup.find_all('a'):
        try:
            link = element['href']
        except KeyError:
            continue
        if _PAGE_LINK_PATTERN.search(link):
            links.append(link)
    return links

_IMG_LINK_PATTERN = re.compile(r'exhentai.org/fullimg.php')


def get_img_link(data):
    """Get image link from binary HTML data"""
    soup = BeautifulSoup(data.decode())
    link = None
    for element in soup.find_all('a'):
        try:
            href_val = element['href']
        except KeyError:
            continue
        if _IMG_LINK_PATTERN.search(href_val):
            link = href_val
            break
    if link:
        return link
    else:
        img = soup.find(id='img')
        link = img['src']
        return link

_IMG_NAME_PATTERN = re.compile(r'^(.*?) ::')


def get_img_name(data):
    """Get image name from binary HTML data"""
    soup = BeautifulSoup(data.decode())
    element = soup.find(id="i4")
    match = _IMG_NAME_PATTERN.match(str(element.div.string))
    return match.group(1)


def main():
    """Main function."""
    # pylint: disable=too-many-locals,too-many-statements

    # Setup
    logging.basicConfig(level='DEBUG')
    cookie_jar = CookieJar()
    opener = urllib.request.build_opener(
        urllib.request.HTTPCookieProcessor(cookie_jar))
    urllib.request.install_opener(opener)

    # parser
    parser = argparse.ArgumentParser()
    parser.add_argument('url')
    parser.add_argument('-s', '--start', type=int, default=None)
    parser.add_argument('-e', '--end', type=int, default=None)
    parser.add_argument('--src', action='store_true')
    parser.add_argument('--renumber', action='store_true')
    args = parser.parse_args()

    # Load config
    config = load_config()
    username = config['username']
    passwd = config['passwd']

    # Log in
    urlopen(FORUMS)
    urlopen(LOGIN, urlencode(
        {'UserName': username,
         'PassWord': passwd,
         'x': 0, 'y': 0}).encode())
    for cookie in (
            convert_cookie(x)
            for x in cookie_jar
            if x.domain == '.e-hentai.org'):
        cookie_jar.set_cookie(cookie)

    # Get links
    data = urlopen(args.url).read()
    pages = get_total_pages(data)
    max_items = get_num_items(data)
    max_digits = int(max_items) // 10 + 2  # for renumbering
    renumber_template = "{{:0{}}}".format(max_digits)
    dest_dir = get_name(data)
    print("Downloading {}".format(dest_dir))
    print("Total pages: {}".format(max_items))
    dest_dir = dest_dir.replace('/', ' ')

    # Make dir
    if not os.path.exists(dest_dir):
        os.mkdir(dest_dir)

    # Touch source if option is set
    if args.src:
        with open(os.path.join(dest_dir, 'src'), 'w') as srcfile:
            srcfile.write(args.url)

    # Download
    links = get_item_links(data)
    count = 1
    for page in range(0, pages):
        for link in links:
            if args.start and count < args.start:
                print('Skipped ({})'.format(count))
                count += 1
                continue
            if args.end and count > args.end:
                print('Stopping')
                return
            while True:
                data = urlopen(link).read()
                imgname = get_img_name(data)
                if args.renumber:
                    imgname = renumber_template.format(count) + \
                        os.path.splitext(imgname)[1]
                imglink = get_img_link(data)
                data = urlopen(imglink).read()
                if data.startswith(b'You have exceeded your'):
                    print('Exceeded limit')
                    urlopen(RESET)
                    time.sleep(5)
                else:
                    break
            with open(os.path.join(dest_dir, imgname), 'wb') as imgfile:
                imgfile.write(data)
            print("Downloaded {} ({})".format(imgname, count))
            count += 1
            time.sleep(5)
        if page + 1 < pages:
            data = urlopen(args.url + '?p=' + str(page + 1)).read()
            links = get_item_links(data)
            # time.sleep(10)

if __name__ == '__main__':
    main()
